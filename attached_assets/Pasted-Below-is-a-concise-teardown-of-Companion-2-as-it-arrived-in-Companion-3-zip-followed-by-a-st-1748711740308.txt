Below is a concise teardown of **Companion 2/** as it arrived in `Companion 3.zip`, followed by a step-by-step playbook for (A) fixing the current build & runtime issues and (B) actually “training” the assistant on your personal data.

---

## 1  Why the code fails to run now

| Symptom                                                                                 | Root cause                                                                                                                                                                                                  | Fix                                                                                                                       |
| --------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **Fly logs** show *“does `uvicorn` exist and is it executable?”*                        | `pyproject.toml` is **empty** (`dependencies = []`). When Fly’s buildpack sees a `pyproject` it ignores `requirements.txt`, so none of your packages—FastAPI, Uvicorn, OpenAI, Pinecone—ever get installed. | Easiest: delete `pyproject.toml` **or** populate it with the same deps as `requirements.txt` and add `uvicorn[standard]`. |
| **Module import errors** for `google_calendar`, `analytics`, etc. when launched locally | Those files exist, but you’re starting the app from inside a different CWD, so Python doesn’t find the package-relative imports.                                                                            | Add a real package structure: <br>\`\`\`                                                                                  |
| Companion/                                                                              |                                                                                                                                                                                                             |                                                                                                                           |

````
__init__.py
main.py
google_calendar.py
…``` then run `python -m Companion.main` **or** `uvicorn Companion.main:app …`. |
````

\| **ValueError** *“OPENAI\_API\_KEY and PINECONE\_API\_KEY must be set”* | `.env` absent in the container; env vars not wired in Fly. | 1. Add `env = { OPENAI_API_KEY=..., PINECONE_API_KEY=... }` to `fly.toml`,<br>or 2. `fly secrets set OPENAI_API_KEY=…`. |
\| **Spurious MacOS junk** in the repo (`__MACOSX/`, `.DS_Store`) | Zip created in Finder. | Add a `.gitignore` entry or run `zip -r --exclude …` before committing. |
\| Possible **Pydantic v1/v2 mismatch** down the road | Requirements don’t pin pydantic; FastAPI ≥ 0.111 now uses Pydantic v2. | Pin FastAPI `^0.111`, Pydantic `^2`, or refactor if you need v1. |
\| Requirements duplication (`python-docx`, `smbprotocol` listed twice) | Cosmetic, but can break lockfiles. | Remove duplicates. |

### Minimal fast, local sanity check

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt uvicorn[standard]
export OPENAI_API_KEY=sk-... PINECONE_API_KEY=...
uvicorn main:app --reload
```

If that boots locally, you know the only thing Fly needs is the cleaned-up dependencies and env vars.

---

## 2  How you *should* “train” the assistant

Think of two layers:

| Layer                           | What it teaches                              | Best technique                                                                                                                         |
| ------------------------------- | -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Style / voice**               | How the assistant *sounds* (tone, phrasing). | **OpenAI fine-tune** on small, high-quality conversation pairs (few thousand examples).                                                |
| **Facts about you / your work** | Private knowledge that changes over time.    | **Retrieval-Augmented Generation (RAG)** – index the documents and fetch them at question-time rather than baking them into the model. |

### 2.1  Fine-tuning for voice (optional)

1. **Curate** 200-1 000 chat-style training rows.

   ```jsonl
   {"messages":[
      {"role":"system","content":"You are Michael’s personal production assistant…"},
      {"role":"user","content":"Quick, what ND filter do I need?"},
      {"role":"assistant","content":"At f/2.8 in bright noon sun…"}
   ]}
   ```
2. `openai file create -p "fine-tune" -f voice.jsonl`
3. `openai fine_tuning.jobs.create -t <FILE_ID> -m gpt-3.5-turbo-0125`
4. Store the resulting `MODEL_NAME` as an env var (`COMPANION_VOICE_MODEL`) and call it in your `/chat` route instead of the base model.

> Fine-tuning is best for *style*, not for sizable, frequently changed facts.

### 2.2  RAG for everything you know

Your codebase already imports Pinecone and defines an `embed()` helper—good start. Wire it into an **ingest / retrieve** loop:

```mermaid
flowchart TD
    A[Docs in Drive, Notion, Email, etc.]
    B[Loader scripts] --> C[Chunk & clean text]
    C --> D[OpenAI Embeddings]
    D --> E[Pinecone index "companion-memory"]
    F[User question] --> G[embed(question)]
    G --> E
    E -- top-k vectors --> H[context passages]
    H --> I[chat completion with context + question]
```

#### Step-by-step launcher scripts

```python
# ingest.py  (run after you add new docs)
from loaders import load_notion, load_drive
from openai import OpenAI
from pinecone import Pinecone

chunks = load_notion() + load_drive()
client = OpenAI()
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("companion-memory")

vectors = [
    {"id": f"{i}", "values": client.embeddings.create(
        model="text-embedding-3-small", input=chunk.text
    ).data[0].embedding, "metadata": {"text": chunk.text}}
    for i, chunk in enumerate(chunks)
]
index.upsert(vectors=vectors, namespace="v1")
```

```python
# in main.py, inside your /chat route
context = index.query(
    vector=embed(user_msg), top_k=6, include_metadata=True
).matches
prompt = "\n\n".join([m.metadata["text"] for m in context])
completion = client.chat.completions.create(
    model=os.getenv("COMPANION_VOICE_MODEL", "gpt-4o-mini"),
    messages=[
        {"role":"system","content":f"You are Michael’s assistant. Use the following context when helpful:\n{prompt}"},
        {"role":"user","content":user_msg}
    ]
)
```

Automate **ingest.py** with a daily cron or Fly job so fresh docs get indexed without redeploying.

---

## 3  Recommended project tidy-up

1. **One dependency file** – either keep `requirements.txt` **or** switch to Poetry and fill `pyproject.toml`.
2. **Real package structure** so `import Companion.google_calendar` works anywhere.
3. **`.env` management** – use `python-dotenv` or FastAPI’s `Settings` class to load variables locally, and `fly secrets` in prod.
4. **Logging** – replace bare `except:` in `embed()` with `except openai.OpenAIError as e: logger.error(e); raise`.
5. **Health check** – add `@app.get("/health")` returning `"ok"` so Fly’s check passes quickly.
6. **Repository hygiene** – add `.gitignore`:

   ```
   __pycache__/
   .DS_Store
   __MACOSX/
   node_modules/
   venv/
   ```
7. **Docs** – move `docs/personal_data_sources.md` to a README section; keep a separate `CONTRIBUTING.md` for onboarding future teammates.

---

## 4  Quick next actions

1. **Delete or fix `pyproject.toml`; push & redeploy.**
2. **Verify `uvicorn main:app` now boots locally.**
3. **Decide**: fine-tune for voice, RAG for private facts (or both).

   * If yes to fine-tune → start curating \~1 000 high-quality Q\&A pairs.
   * Regardless → build an `ingest.py` script to push your Notion pages, GCal events, etc. into Pinecone.
4. **Schedule** a daily Fly cron or GitHub Action to run `python ingest.py`.
5. **Add a usage log** (SQLite or Supabase) so you can review questions the model misses and iteratively improve your data.

Work through those in order and you’ll have a Companion that (a) actually runs, (b) sounds like you, and (c) can pull up anything you’ve ever written or shot. Let me know where you hit friction and we’ll tackle the next piece together.
